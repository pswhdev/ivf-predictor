{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Fit and evaluate a cluster model to group similar data\n",
    "* Understand the profile for each cluster\n",
    "\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* outputs/datasets/cleaned/FertilityTreatmentDataCleaned.csv\n",
    "* Instructions on which variables to use for data cleaning and feature engineering, found in their respective notebooks.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Cluster Pipeline\n",
    "* Train Set\n",
    "* Most important features to define a cluster plot\n",
    "* Clusters Profile Description\n",
    "* Cluster Silhouette\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the working directory from its current folder to its parent folder\n",
    "* Access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the parent of the current directory the new current directory:\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"A new current directory has been set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Open dataset\n",
    "df = pd.read_csv(\"outputs/datasets/cleaned/FertilityTreatmentDataCleaned.csv\")\n",
    "        \n",
    "print(df.shape)\n",
    "df.head(3)\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline with all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from src.custom_transformers import OrdinalEncoderWithCategories\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Define the natural order for each feature\n",
    "categories = [\n",
    "    ['18-34', '35-37', '38-39', '40-42', '43-44', '45-50'],  # Patient age at treatment\n",
    "    ['0', '1', '2', '3', '4', '5', '>5'],                    # Total number of previous IVF cycles\n",
    "    ['18-34', '35-37', '38-39', '40-42', '43-44', '45-50'],  # Patient/Egg provider age\n",
    "    ['18-34', '35-37', '38-39', '40-42', '43-44', '45-50', '51-55', '56-60', '>60'],  # Partner/Sperm provider age\n",
    "    ['0', '0 - frozen cycle', '1-5', '6-10', '11-15', '16-20', '21-25', '26-30', '31-35', '36-40', '>40'], # Fresh eggs collected\n",
    "    ['0', '0 - frozen cycle', '1-5', '6-10', '11-15', '16-20', '21-25', '26-30', '31-35', '36-40', '>40'], # Total eggs mixed\n",
    "    ['0', '0 - frozen cycle', '1-5', '6-10', '11-15', '16-20', '21-25', '26-30', '>30'],  # Total embryos created\n",
    "    ['0', '1', '1e', '2', '3'],  # Embryos transferred\n",
    "    ['0 - fresh cycle', '0 - frozen cycle', '1-5', '6-10', '>10']  # Total embryos thawed\n",
    "]\n",
    "\n",
    "# Define columns that need ordinal encoding\n",
    "ordinal_columns = [\n",
    "    'Patient age at treatment',\n",
    "    'Total number of previous IVF cycles',\n",
    "    'Patient/Egg provider age',\n",
    "    'Partner/Sperm provider age',\n",
    "    'Fresh eggs collected',\n",
    "    'Total eggs mixed',\n",
    "    'Total embryos created',\n",
    "    'Embryos transferred',\n",
    "    'Total embryos thawed'\n",
    "]\n",
    "\n",
    "def PipelineFECluster():\n",
    "    pipeline_base = Pipeline(\n",
    "        [   \n",
    "            ('ordinal_encoding', OrdinalEncoderWithCategories(\n",
    "                categories=categories,\n",
    "                columns=ordinal_columns\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                \"one_hot_encoding\",\n",
    "                OneHotEncoder(\n",
    "                    variables=[\n",
    "                        \"Specific treatment type\",\n",
    "                        \"Egg source\",\n",
    "                        \"Sperm source\",\n",
    "                        \"Patient ethnicity\",\n",
    "                        \"Date of embryo transfer\"\n",
    "                    ],\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"smart_correlation\",\n",
    "                SmartCorrelatedSelection(\n",
    "                    method=\"spearman\",\n",
    "                    threshold=0.9,\n",
    "                    selection_method=\"variance\",\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering pipeline:\n",
    "\n",
    "Pipeline for scaling, PCA and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineCluster():\n",
    "    pipeline_base = Pipeline(\n",
    "        [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "            # Leave at 50 components for now\n",
    "            (\"pca\", PCA(n_components=50, random_state=0)),\n",
    "            # Leave at 50 clusters for now\n",
    "            (\"model\", KMeans(n_clusters=50, random_state=0))\n",
    "        ]\n",
    "    )\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_cluster = PipelineFECluster()\n",
    "df = pre_processing_cluster.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_pca = Pipeline(pipeline_cluster.steps[:-2])\n",
    "df_pca = pipeline_pca.fit_transform(df)\n",
    "\n",
    "print(df_pca.shape,'\\n', type(df_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA separately to the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "# Set the number of components to the number of features\n",
    "n_components = 54\n",
    "\n",
    "\n",
    "def pca_components_analysis(df_pca, n_components):\n",
    "    pca = PCA(n_components=n_components).fit(df_pca)\n",
    "    x_PCA = pca.transform(df_pca)  # array with transformed PCA\n",
    "\n",
    "    ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
    "    dfExplVarRatio = pd.DataFrame(\n",
    "        data=np.round(100 * pca.explained_variance_ratio_, 3),\n",
    "        index=ComponentsList,\n",
    "        columns=[\"Explained Variance Ratio (%)\"],\n",
    "    )\n",
    "\n",
    "    dfExplVarRatio[\"Accumulated Variance\"] = dfExplVarRatio[\n",
    "        \"Explained Variance Ratio (%)\"\n",
    "    ].cumsum()\n",
    "\n",
    "    PercentageOfDataExplained = dfExplVarRatio[\"Explained Variance Ratio (%)\"].sum()\n",
    "\n",
    "    print(\n",
    "        f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\"\n",
    "    )\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.lineplot(data=dfExplVarRatio, marker=\"o\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(np.arange(0, 110, 10))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "pca_components_analysis(df_pca=df_pca, n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components_analysis(df_pca=df_pca,n_components=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineCluster():\n",
    "    pipeline_base = Pipeline(\n",
    "        [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "            # Apply PCA with chosen number of components\n",
    "            (\"pca\", PCA(n_components=27, random_state=0)),\n",
    "            # Leave at 50 clusters for now\n",
    "            (\"model\", KMeans(n_clusters=50, random_state=0))\n",
    "        ]\n",
    "    )\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method and Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
    "df_analysis = pipeline_analysis.fit_transform(df)\n",
    "print(df_analysis.shape,'\\n', type(df_analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11)) # 11 is not inclusive, it will plot until 10\n",
    "visualizer.fit(df_analysis) \n",
    "visualizer.show() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "# 8 is not inclusive, it will stop at 6\n",
    "n_cluster_start, n_cluster_stop = 2, 7\n",
    "\n",
    "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
    "visualizer = KElbowVisualizer(\n",
    "    KMeans(random_state=0), k=(n_cluster_start, n_cluster_stop), metric=\"silhouette\"\n",
    ")\n",
    "visualizer.fit(df_analysis)\n",
    "visualizer.show()\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
    "\n",
    "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
    "    visualizer = SilhouetteVisualizer(\n",
    "        estimator=KMeans(n_clusters=n_clusters, random_state=0), colors=\"yellowbrick\"\n",
    "    )\n",
    "    visualizer.fit(df_analysis)\n",
    "    visualizer.show()\n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Cluster Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineCluster():\n",
    "    pipeline_base = Pipeline(\n",
    "        [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "            # Apply PCA with chosen number of components\n",
    "            (\"pca\", PCA(n_components=27, random_state=0)),\n",
    "            # Leave at 50 clusters for now\n",
    "            (\"model\", KMeans(n_clusters=4, random_state=0))\n",
    "        ]\n",
    "    )\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the data frame already encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "print(X.shape)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the clustering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_cluster.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add cluster predictions to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column \"`Clusters`\" (with the cluster pipeline predictions) to the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Clusters'] = pipeline_cluster['model'].labels_\n",
    "print(X.shape)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
    "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# df_analysis is returnded as a numpy array, so we need to convert it back to a DataFrame\n",
    "df_analysis = pd.DataFrame(df_analysis)\n",
    "\n",
    "# Set the style for seaborn plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a figure with specified size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Correctly indexing the DataFrame to plot scatter points\n",
    "sns.scatterplot(\n",
    "    # Use iloc to access the first column\n",
    "    x=df_analysis.iloc[:, 0],\n",
    "    # Use iloc to access the second column\n",
    "    y=df_analysis.iloc[:, 1],  \n",
    "    hue=X['Clusters'],\n",
    "    palette='Set1',\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Plot cluster centers\n",
    "plt.scatter(\n",
    "    x=pipeline_cluster['model'].cluster_centers_[:, 0], \n",
    "    y=pipeline_cluster['model'].cluster_centers_[:, 1],\n",
    "    marker=\"x\", \n",
    "    s=169, \n",
    "    linewidths=3, \n",
    "    color=\"black\"\n",
    ")\n",
    "\n",
    "# Set labels and title for the plot\n",
    "plt.xlabel(\"PCA Component 0\")\n",
    "plt.ylabel(\"PCA Component 1\")\n",
    "plt.title(\"PCA Components Colored by Clusters\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a 3D interactive plot for better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go  # Import graph_objects for custom traces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ensure 'Clusters' column is added and matches the data correctly\n",
    "df_analysis[\"Clusters\"] = X[\"Clusters\"]\n",
    "\n",
    "# Select the first three columns and add 'Clusters' for plotting\n",
    "# Adjust these column indices if your PCA components are located elsewhere\n",
    "selected_columns = df_analysis.iloc[\n",
    "    :, :3\n",
    "]  # Select first three components (adjust indices if needed)\n",
    "selected_columns.columns = [\n",
    "    \"PCA Component 0\",\n",
    "    \"PCA Component 1\",\n",
    "    \"PCA Component 2\",\n",
    "]  # Rename selected columns\n",
    "selected_columns[\"Clusters\"] = df_analysis[\"Clusters\"]  # Add the clusters column\n",
    "\n",
    "# Create an interactive 3D scatter plot with Plotly\n",
    "fig = px.scatter_3d(\n",
    "    selected_columns,\n",
    "    x=\"PCA Component 0\",  # Reference by column name\n",
    "    y=\"PCA Component 1\",  # Reference by column name\n",
    "    z=\"PCA Component 2\",  # Reference by column name\n",
    "    color=\"Clusters\",  # Color by cluster labels\n",
    "    title='Interactive 3D PCA Components Colored by Clusters',\n",
    "    color_continuous_scale='Viridis',\n",
    "    opacity=0.7,\n",
    ")\n",
    "\n",
    "# Update the trace for data points to adjust marker size\n",
    "fig.update_traces(marker=dict(size=4))  # Adjust size to desired level\n",
    "\n",
    "# Extract cluster centers from the clustering model for plotting\n",
    "cluster_centers = pipeline_cluster[\"model\"].cluster_centers_\n",
    "\n",
    "# Add cluster centers to the plot\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=cluster_centers[:, 0],\n",
    "        y=cluster_centers[:, 1],\n",
    "        z=cluster_centers[:, 2],\n",
    "        mode=\"markers\",\n",
    "        # Adjust size here to change cluster centers size\n",
    "        marker=dict(size=10, color=\"black\", symbol=\"x\"),\n",
    "        name=\"Cluster Centers\",\n",
    "    )\n",
    ")\n",
    "# Update layout to improve figure size, legend, and color bar placement\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='PCA Component 0',\n",
    "        yaxis_title='PCA Component 1',\n",
    "        zaxis_title='PCA Component 2',\n",
    "    ),\n",
    "    width=1000,  \n",
    "    height=800,  \n",
    "    legend=dict(\n",
    "        orientation=\"h\",  # Horizontal orientation for the legend\n",
    "        yanchor=\"top\",\n",
    "        y=1.05,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5\n",
    "    ),\n",
    "    coloraxis_colorbar=dict(\n",
    "        title=\"Clusters\",\n",
    "        x=1.05,  # Adjust the horizontal position of the color bar\n",
    "        thickness=20,  # Adjust the thickness of the color bar\n",
    "    )\n",
    ")\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cluster predictions from this pipeline to use in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_predictions_with_all_variables = X['Clusters']\n",
    "cluster_predictions_with_all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a classifier, where the target is cluster predictions and features remaining variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy `X` to a DataFrame `df_clf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf = X.copy()\n",
    "print(df_clf.shape)\n",
    "df_clf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_clf.drop(['Clusters'], axis=1),\n",
    "    df_clf['Clusters'],\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create classifier pipeline steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GradientBoostingClassifier as the algorithm since it typically has good performance while being fast to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# ML algorithm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def PipelineClf2ExplainClusters():\n",
    "    pipeline_base = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\n",
    "            \"feat_selection\",\n",
    "            SelectFromModel(GradientBoostingClassifier(random_state=0)),\n",
    "            ),\n",
    "            (\"model\", GradientBoostingClassifier(random_state=0)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipeline_base\n",
    "\n",
    "\n",
    "PipelineClf2ExplainClusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the classifier to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
    "pipeline_clf_cluster.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate classifier performance on Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print classification report for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Training Set Classification Report:\")\n",
    "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print classification report for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Set Classification Report:\")\n",
    "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess the most important Features that define a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_after_data_cleaning_feat_eng = df.columns\n",
    "\n",
    "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support(\n",
    ")].to_list()\n",
    "\n",
    "# Create DataFrame to display feature importance from the model\n",
    "df_feature_importance = pd.DataFrame(\n",
    "    {\n",
    "        \"Feature\": best_features,\n",
    "        \"Importance\": pipeline_clf_cluster[\"model\"].feature_importances_,\n",
    "    }\n",
    ").sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Reassign best features in order of importance\n",
    "best_features = df_feature_importance[\"Feature\"].to_list()\n",
    "\n",
    "# Display the most important features and plot their importance\n",
    "print(\n",
    "    f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "    f\"The model was trained on them: \\n{best_features} \\n\"\n",
    ")\n",
    "\n",
    "# Plotting feature importances\n",
    "df_feature_importance.plot(kind=\"bar\", x=\"Feature\", y=\"Importance\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the best_features to use at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_pipeline_all_variables = best_features\n",
    "best_features_pipeline_all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load function that plots a table with description for all Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DescriptionAllClusters(df, decimal_points=3):\n",
    "\n",
    "    DescriptionAllClusters = pd.DataFrame(\n",
    "        columns=df.drop(['Clusters'], axis=1).columns)\n",
    "    # iterate on each cluster , calls Clusters_IndividualDescription()\n",
    "    for cluster in df.sort_values(by='Clusters')['Clusters'].unique():\n",
    "\n",
    "        EDA_ClusterSubset = df.query(\n",
    "            f\"Clusters == {cluster}\").drop(['Clusters'], axis=1)\n",
    "        ClusterDescription = Clusters_IndividualDescription(\n",
    "            EDA_ClusterSubset, cluster, decimal_points)\n",
    "        DescriptionAllClusters = DescriptionAllClusters.append(\n",
    "            ClusterDescription)\n",
    "\n",
    "    DescriptionAllClusters.set_index(['Cluster'], inplace=True)\n",
    "    return DescriptionAllClusters\n",
    "\n",
    "\n",
    "def Clusters_IndividualDescription(EDA_Cluster, cluster, decimal_points):\n",
    "\n",
    "    ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
    "    # for a given cluster, iterate over all columns\n",
    "    # if the variable is numerical, calculate the IQR: display as Q1 -- Q3.\n",
    "    # That will show the range for the most common values for the numerical variable\n",
    "    # if the variable is categorical, count the frequencies and displays the top 3 most frequent\n",
    "    # That will show the most common levels for the category\n",
    "\n",
    "    for col in EDA_Cluster.columns:\n",
    "\n",
    "        try:  # eventually a given cluster will have only missing data for a given variable\n",
    "\n",
    "            if EDA_Cluster[col].dtypes == 'object':\n",
    "\n",
    "                top_frequencies = EDA_Cluster.dropna(\n",
    "                    subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
    "                Description = ''\n",
    "\n",
    "                for x in range(len(top_frequencies)):\n",
    "                    freq = top_frequencies.iloc[x]\n",
    "                    category = top_frequencies.index[x][0]\n",
    "                    CategoryPercentage = int(round(freq*100, 0))\n",
    "                    statement = f\"'{category}': {CategoryPercentage}% , \"\n",
    "                    Description = Description + statement\n",
    "\n",
    "                ClustersDescription.at[0, col] = Description[:-2]\n",
    "\n",
    "            elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
    "                DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
    "                Q1 = round(DescStats.iloc[4, 0], decimal_points)\n",
    "                Q3 = round(DescStats.iloc[6, 0], decimal_points)\n",
    "                Description = f\"{Q1} -- {Q3}\"\n",
    "                ClustersDescription.at[0, col] = Description\n",
    "\n",
    "        except Exception as e:\n",
    "            ClustersDescription.at[0, col] = 'Not available'\n",
    "            print(\n",
    "                f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
    "\n",
    "    ClustersDescription['Cluster'] = str(cluster)\n",
    "\n",
    "    return ClustersDescription\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a custom function to plot cluster distribution per Variable (absolute and relative levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def cluster_distribution_per_variable(df, target):\n",
    "    \"\"\"\n",
    "    The data should have 2 variables, the cluster predictions and\n",
    "    the variable you want to analyze with, in this case we call \"target\".\n",
    "    We use plotly express to create 2 plots:\n",
    "    Cluster distribution across the target.\n",
    "    Relative presence of the target level in each cluster.\n",
    "    \"\"\"\n",
    "    df_bar_plot = df.value_counts([\"Clusters\", target]).reset_index()\n",
    "    df_bar_plot.columns = [\"Clusters\", target, \"Count\"]\n",
    "    df_bar_plot[target] = df_bar_plot[target].astype(\"object\")\n",
    "\n",
    "    print(f\"Clusters distribution across {target} levels\")\n",
    "    fig = px.bar(\n",
    "        df_bar_plot, x=\"Clusters\", y=\"Count\", color=target, width=800, height=500\n",
    "    )\n",
    "    fig.update_layout(xaxis=dict(tickmode=\"array\", tickvals=df[\"Clusters\"].unique()))\n",
    "    fig.show(renderer=\"jupyterlab\")\n",
    "\n",
    "    df_relative = (\n",
    "        df.groupby([\"Clusters\", target])\n",
    "        .size()\n",
    "        .groupby(level=0)\n",
    "        .apply(lambda x: 100 * x / x.sum())\n",
    "        .reset_index()\n",
    "        .sort_values(by=[\"Clusters\"])\n",
    "    )\n",
    "    df_relative.columns = [\"Clusters\", target, \"Relative Percentage (%)\"]\n",
    "\n",
    "    print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
    "    fig = px.line(\n",
    "        df_relative,\n",
    "        x=\"Clusters\",\n",
    "        y=\"Relative Percentage (%)\",\n",
    "        color=target,\n",
    "        width=800,\n",
    "        height=500,\n",
    "    )\n",
    "    fig.update_layout(xaxis=dict(tickmode=\"array\", tickvals=df[\"Clusters\"].unique()))\n",
    "    fig.update_traces(mode=\"markers+lines\")\n",
    "    fig.show(renderer=\"jupyterlab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame that contains best features and Clusters Predictions since we want to analyse the patterns for each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_profile = df_clf.copy()\n",
    "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
    "print(df_cluster_profile.shape)\n",
    "df_cluster_profile.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want also to analyse Live birth occurrence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_success = pd.read_csv(\"outputs/datasets/cleaned/FertilityTreatmentDataCleaned.csv\").filter(['Live birth occurrence'])\n",
    "df_success['Live birth occurrence'] = df_success['Live birth occurrence'].astype('object')\n",
    "df_success.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster profile based on the best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set display options to show the full content of each column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Combine the DataFrames and ensure proper alignment\n",
    "# Make sure df_cluster_profile and df_success have compatible indices or columns\n",
    "combined_df = pd.concat([df_cluster_profile, df_success], axis=1)\n",
    "\n",
    "# Check the first few rows to ensure proper concatenation\n",
    "print(combined_df.head())\n",
    "\n",
    "# Ensure that DescriptionAllClusters is defined properly and can handle the combined DataFrame\n",
    "# Provide the combined DataFrame to the description function\n",
    "clusters_profile = DescriptionAllClusters(df=combined_df, decimal_points=0)\n",
    "\n",
    "# Display the clusters profile\n",
    "clusters_profile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters distribution across Live birth occurrence levels & Relative Percentage of Live birth occurrence in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_vs_success =  df_success.copy()\n",
    "df_cluster_vs_success['Clusters'] = X['Clusters']\n",
    "cluster_distribution_per_variable(df=df_cluster_vs_success, target='Live birth occurrence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit New Cluster Pipeline with most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce feature space, we will study the trade-off between the previous Cluster Pipeline (fitted with all variables) and Pipeline using the variables that are most important to define the clusters from the previous pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_pipeline_all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reassign best features names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding create new columns and appends the value after the name as `_<value>`.\n",
    "Therefore it is necessary to rename the features to the original name, so when the pipeline is excecuted, it will find the correct columns on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def reassign_best_features(features):\n",
    "    \"\"\"\n",
    "    Reassigns encoded feature names to their original names by removing any suffixes added by one-hot encoding.\n",
    "    \"\"\"\n",
    "    # Use regex to remove the \"_<value>\" suffix from feature names\n",
    "    reassigned_best_features = [re.sub(r'_[^_]+$', '', feature) for feature in features]\n",
    "    # Filter out duplicate values from the reassigned features list\n",
    "    unique_reassigned_best_features = list(set(reassigned_best_features))\n",
    "\n",
    "    return unique_reassigned_best_features\n",
    "\n",
    "reassigned_best_features = reassign_best_features(best_features)\n",
    "reassigned_best_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define trade-off and metrics to compare new and previous Cluster Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate this trade-off:\n",
    "1. Conduct a elbow method and silhouette analysis and check if the same number of clusters is suggested\n",
    "2. Fit new cluster pipeline and compare if the predictions from this pipeline are \"equivalent\" to the predictions from the previous pipeline\n",
    "3. Fit a classifier to explain cluster, and check if performance on Train and Test sets is similar to the previous pipeline\n",
    "4. Check if the most important features for the classifier are the same from the previous pipeline\n",
    "5. Compare if the cluster profile from both pipelines are \"equivalent\"\n",
    "\n",
    "If yes, a cluster pipeline using the features that best define the clusters from previous pipeline can be used.\n",
    "* In real-time fewer variables are needed for predicting clusters for the prospect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset data with the most relevant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"outputs/datasets/cleaned/FertilityTreatmentDataCleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df.filter(reassigned_best_features)\n",
    "df_reduced.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite Cluster Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the Feature Engineering Pipeline, considering only the most important variables from the previous pipeline: `['Date of embryo transfer',\n",
    " 'Embryos transferred from eggs micro-injected',\n",
    " 'Total embryos thawed',\n",
    " 'Patient age at treatment']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the natural order for each feature\n",
    "categories = [\n",
    "    ['18-34', '35-37', '38-39', '40-42', '43-44', '45-50'],  # Patient age at treatment\n",
    "    ['0 - fresh cycle', '0 - frozen cycle', '1-5', '6-10', '>10']  # Total embryos thawed\n",
    "]\n",
    "\n",
    "# Define columns that need ordinal encoding\n",
    "ordinal_columns = [\n",
    "    'Patient age at treatment',\n",
    "    'Total embryos thawed'\n",
    "]\n",
    "\n",
    "\n",
    "def PipelineFECluster():\n",
    "    pipeline_base = Pipeline(\n",
    "        [   # Use only the best features\n",
    "            ('ordinal_encoding', OrdinalEncoderWithCategories(\n",
    "                categories=categories,\n",
    "                columns=ordinal_columns\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                \"one_hot_encoding\",\n",
    "                OneHotEncoder(\n",
    "                    variables=[\n",
    "                        \"Date of embryo transfer\"\n",
    "                    ],\n",
    "                ),\n",
    "            ),\n",
    "            # it doesn't need SmartCorrelationSelection\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipeline_base\n",
    "\n",
    "PipelineFECluster()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Elbow Method and Silhouette analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_analysis = PipelineFECluster()\n",
    "df_analysis = pipeline_analysis.fit_transform(df_reduced)\n",
    "\n",
    "print(df_analysis.shape,'\\n', type(df_analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11))\n",
    "visualizer.fit(df_analysis) \n",
    "visualizer.show() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "n_cluster_start, n_cluster_stop = 2, 6\n",
    "\n",
    "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(\n",
    "    n_cluster_start, n_cluster_stop), metric='silhouette')\n",
    "visualizer.fit(df_analysis)\n",
    "visualizer.show()\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
    "\n",
    "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
    "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, random_state=0),\n",
    "                                      colors='yellowbrick')\n",
    "    visualizer.fit(df_analysis)\n",
    "    visualizer.show()\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit New Cluster Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set X as our training set for the cluster. It is a copy of df_reduced already passed through feature engineering processes (already encoded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reduced.copy()\n",
    "print(X.shape)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit Cluster pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_cluster.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add cluster predictions to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a column \"`Clusters`\" (with the cluster pipeline predictions) to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Clusters'] = pipeline_cluster['model'].labels_\n",
    "print(X.shape)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
    "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare current cluster predictions to previous cluster predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just fitted a new cluster pipeline and want to compare if its predictions are \"equivalent\" to the previous cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the predictions from the **previous** cluster pipeline - trained with all variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_predictions_with_all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are the predictions from **current** cluster pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_predictions_with_best_features = X['Clusters'] \n",
    "cluster_predictions_with_best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a confusion matrix to evaluate if the predictions of both pipelines are **\"equivalent\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(cluster_predictions_with_all_variables, cluster_predictions_with_best_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a classifier, where the target is cluster predictions and features remaining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf = X.copy()\n",
    "print(df_clf.shape)\n",
    "df_clf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_clf.drop(['Clusters'], axis=1),\n",
    "    df_clf['Clusters'],\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite pipeline to explain clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineClf2ExplainClusters():\n",
    "    pipeline_base = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            # no need to consider feature selection step, since we know which features to consider\n",
    "            (\"model\", GradientBoostingClassifier(random_state=0)),\n",
    "        ]\n",
    "    )\n",
    "    return pipeline_base\n",
    "\n",
    "\n",
    "PipelineClf2ExplainClusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a classifier, where the target is cluster labels and features remaining variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and fit a classifier pipeline to learn the feature importance when defining a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
    "pipeline_clf_cluster.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate classifier performance on Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print classification report for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Training Set Classification Report:\")\n",
    "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print classification report for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Set Classification Report:\")\n",
    "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we don't have feature selection step in this pipeline, best_features is Xtrain columns\n",
    "best_features = X_train.columns.to_list()\n",
    "\n",
    "# create a DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "    'Feature': best_features,\n",
    "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
    "    .sort_values(by='Importance', ascending=False)\n",
    ")\n",
    "\n",
    "best_features = df_feature_importance['Feature'].to_list()\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
    "\n",
    "# Plotting feature importances\n",
    "df_feature_importance.plot(kind=\"bar\", x=\"Feature\", y=\"Importance\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame that contains the best features and Clusters Predictions: we want to analyse the patterns for each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_profile = df_clf.copy()\n",
    "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
    "print(df_cluster_profile.shape)\n",
    "df_cluster_profile.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want also to analyse Live birth occurence levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_success = pd.read_csv(\"outputs/datasets/cleaned/FertilityTreatmentDataCleaned.csv\").filter(['Live birth occurrence'])\n",
    "df_success['Live birth occurrence'] = df_success['Live birth occurrence'].astype('object')\n",
    "df_success.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster profile on most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "clusters_profile = DescriptionAllClusters(df= pd.concat([df_cluster_profile,df_success], axis=1), decimal_points=0)\n",
    "clusters_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters distribution across Live birth occurrence & Relative Percentage of Live birth occurrence in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_vs_success=  df_success.copy()\n",
    "df_cluster_vs_success['Clusters'] = X['Clusters']\n",
    "cluster_distribution_per_variable(df=df_cluster_vs_success, target='Live birth occurrence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which pipeline to deploy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap the criteria we consider to evaluate the **trade-off**\n",
    "1. Conduct an elbow method and silhouette analysis and check if the same number of clusters is suggested.\n",
    "2. Fit a new cluster pipeline and compare if the predictions from this pipeline are \"equivalent\" to the predictions from the previous pipeline.\n",
    "3. Fit a classifier to explain cluster and check if performance on Train and Test sets is similar to the previous pipeline.\n",
    "4. Check if the most important features for the classifier are the same from the previous pipeline.\n",
    "5. Compare if the cluster profile from both pipelines is \"equivalent\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push files to Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will generate the following files\n",
    "\n",
    "* Cluster Pipeline\n",
    "* Train Set\n",
    "* Feature importance plot\n",
    "* Clusters Description\n",
    "* Cluster Silhouette\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "version = 'v1'\n",
    "file_path = f'outputs/ml_pipeline/cluster_analysis/{version}'\n",
    "\n",
    "try:\n",
    "    os.makedirs(name=file_path)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(value=pipeline_cluster, filename=f\"{file_path}/cluster_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_reduced.shape)\n",
    "df_reduced.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.to_csv(f\"{file_path}/TrainSet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most important features plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the features that define a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\n",
    "plt.savefig(f\"{file_path}/features_define_cluster.png\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_profile.to_csv(f\"{file_path}/clusters_profile.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster silhouette plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick')\n",
    "visualizer.fit(df_analysis)\n",
    "visualizer.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(7,5))\n",
    "fig = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick', ax=axes)\n",
    "fig.fit(df_analysis)\n",
    "\n",
    "plt.savefig(f\"{file_path}/clusters_silhouette.png\", bbox_inches='tight',dpi=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
